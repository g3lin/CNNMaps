{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_train_resnet.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNKbQZlMWOf+bUPIpu5OX2r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"236d5e535b9e4571bf70b772be8d7dfc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_893880237859479296cdb4335eeee2e8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9b9066c81286463da7513a8e891aa086","IPY_MODEL_3b81e69dd52d40ea9b630edc270f5de4"]}},"893880237859479296cdb4335eeee2e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b9066c81286463da7513a8e891aa086":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5aea52137ccd474580d0f49ca55c948f","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_878af4bfe11f4ef894d84f90fa4dbd03"}},"3b81e69dd52d40ea9b630edc270f5de4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be836b17e119434a8a844026f1f28b3c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:02&lt;00:00, 20.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_760902fdbe8c484280105e1828b2cade"}},"5aea52137ccd474580d0f49ca55c948f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"878af4bfe11f4ef894d84f90fa4dbd03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be836b17e119434a8a844026f1f28b3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"760902fdbe8c484280105e1828b2cade":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"Iq2VHvIERO58","colab_type":"code","outputId":"82b55cf7-f497-4136-dc40-d0c5deb22634","executionInfo":{"status":"ok","timestamp":1586109521817,"user_tz":-120,"elapsed":20142,"user":{"displayName":"Julien Brosseau","photoUrl":"https://lh5.googleusercontent.com/-Uack_F0uJ0c/AAAAAAAAAAI/AAAAAAAAAHk/fiTPiqUiz0A/s64/photo.jpg","userId":"00920853515232236248"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ep1TjHrfgNCl","colab_type":"code","outputId":"5e5a37fe-09dc-49ef-ef4e-39381045b359","executionInfo":{"status":"ok","timestamp":1586109535511,"user_tz":-120,"elapsed":33819,"user":{"displayName":"Julien Brosseau","photoUrl":"https://lh5.googleusercontent.com/-Uack_F0uJ0c/AAAAAAAAAAI/AAAAAAAAAHk/fiTPiqUiz0A/s64/photo.jpg","userId":"00920853515232236248"}},"colab":{"base_uri":"https://localhost:8080/","height":673}},"source":["!pip install tifffile\n","!pip install segmentation-models-pytorch"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tifffile\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/e8/76979051a2a15b23e2652a3cdeb7d3dfec518c54138bd0f8b5a5ef2c267a/tifffile-2020.2.16-py3-none-any.whl (130kB)\n","\u001b[K     |████████████████████████████████| 133kB 4.9MB/s \n","\u001b[?25hCollecting imagecodecs>=2020.1.31\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c0/a2001ce40ddf86f5944a111d61a8bb2799d10e169ba47979bc6c821cf8be/imagecodecs-2020.2.18-cp36-cp36m-manylinux2014_x86_64.whl (18.1MB)\n","\u001b[K     |████████████████████████████████| 18.1MB 196kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.6/dist-packages (from tifffile) (1.18.2)\n","Installing collected packages: imagecodecs, tifffile\n","Successfully installed imagecodecs-2020.2.18 tifffile-2020.2.16\n","Collecting segmentation-models-pytorch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/88/763a25dfe076a9f30f33466b1bd0f2d31b915b88d4cb4481fe4043cf26b4/segmentation_models_pytorch-0.1.0-py3-none-any.whl (42kB)\n","\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch) (0.5.0)\n","Collecting pretrainedmodels==0.7.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n","\u001b[?25hCollecting efficientnet-pytorch>=0.5.1\n","  Downloading https://files.pythonhosted.org/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.18.2)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (7.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.12.0)\n","Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.4.0)\n","Collecting munch\n","  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.38.0)\n","Building wheels for collected packages: pretrainedmodels, efficientnet-pytorch\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=35f27e67dcb9774a455954c0f732009556a6bc439265c4f55ef311785b322622\n","  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-cp36-none-any.whl size=12422 sha256=e684a488c31c5f03990d7903c7aa7476e51d2f7d403f9a31be0dcfdfd8b76e8b\n","  Stored in directory: /root/.cache/pip/wheels/42/1e/a9/2a578ba9ad04e776e80bf0f70d8a7f4c29ec0718b92d8f6ccd\n","Successfully built pretrainedmodels efficientnet-pytorch\n","Installing collected packages: munch, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n","Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"muWYOHf5RQnR","colab_type":"code","outputId":"35b7664e-66b4-4a4d-c238-00e099124fac","executionInfo":{"status":"ok","timestamp":1586109535519,"user_tz":-120,"elapsed":33817,"user":{"displayName":"Julien Brosseau","photoUrl":"https://lh5.googleusercontent.com/-Uack_F0uJ0c/AAAAAAAAAAI/AAAAAAAAAHk/fiTPiqUiz0A/s64/photo.jpg","userId":"00920853515232236248"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cd /content/drive/My Drive/Projet SatMap/Implementation"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Projet SatMap/Implementation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YehQ_UdmQ9es","colab_type":"code","outputId":"e8c00fcd-bf64-468f-b693-b00738533255","executionInfo":{"status":"error","timestamp":1586109558745,"user_tz":-120,"elapsed":57035,"user":{"displayName":"Julien Brosseau","photoUrl":"https://lh5.googleusercontent.com/-Uack_F0uJ0c/AAAAAAAAAAI/AAAAAAAAAHk/fiTPiqUiz0A/s64/photo.jpg","userId":"00920853515232236248"}},"colab":{"base_uri":"https://localhost:8080/","height":312,"referenced_widgets":["236d5e535b9e4571bf70b772be8d7dfc","893880237859479296cdb4335eeee2e8","9b9066c81286463da7513a8e891aa086","3b81e69dd52d40ea9b630edc270f5de4","5aea52137ccd474580d0f49ca55c948f","878af4bfe11f4ef894d84f90fa4dbd03","be836b17e119434a8a844026f1f28b3c","760902fdbe8c484280105e1828b2cade"]}},"source":["# Define the helper function\n","def decode_segmap(image, nc=21):\n","  \n","  label_colors = np.array([(0, 0, 0),  # 0=background\n","               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n","               (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n","               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n","               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n","               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n","               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n","               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n","               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n","\n","  r = np.zeros_like(image).astype(np.uint8)\n","  g = np.zeros_like(image).astype(np.uint8)\n","  b = np.zeros_like(image).astype(np.uint8)\n","  \n","  for l in range(0, nc):\n","    idx = image == l\n","    r[idx] = label_colors[l, 0]\n","    g[idx] = label_colors[l, 1]\n","    b[idx] = label_colors[l, 2]\n","    \n","  rgb = np.stack([r, g, b], axis=2)\n","  return rgb\n","\n","# Liste des images annotees\n","LIST_IMG_ID = ('6010_1_2', '6010_4_2', '6010_4_4', '6040_1_0', '6040_1_3', '6040_2_2', \n","         '6040_4_4', '6060_2_3', '6070_2_3', '6090_2_0', '6100_1_3', '6100_2_2',\n","         '6100_2_3', '6110_1_2', '6110_3_1', '6110_4_0', '6120_2_0', '6120_2_2',\n","         '6140_1_2', '6140_3_1', '6150_2_3', '6160_2_1', '6170_0_4', '6170_2_4',\n","         '6170_4_1')\n","# ID des classes respectives : batiments, routes, arbres, rivieres, lacs \n","LIST_POLY_TYPE = ('1', '3', '5', '7', '8')\n","IMG_ID = LIST_IMG_ID[17]\n","POLY_TYPE = LIST_POLY_TYPE[0]\n","\n","from torchvision import transforms, datasets, models\n","import torch\n","from torch.autograd import Variable\n","import torch.nn as nn\n","\n","import segmentation_models_pytorch as smp\n","\n","import numpy as np\n","\n","from PIL import Image\n","import shapely.affinity\n","\n","import tifffile as tiff\n","\n","import matplotlib.pyplot as plt\n","\n","# Ouverture des fichiers\n","import bin.data_opening as op\n","\n","data_opening = op.DataOpening()\n","\n","#img = Image.open('./data/three_band/6120_2_2.tif')\n","#img_size = img.size\n","img, img_size = data_opening.get_tiff(IMG_ID)\n","\n","x_max, y_min = data_opening.get_size(IMG_ID)\n","train_polygons = data_opening.get_polygons(IMG_ID, POLY_TYPE)\n","\n","# Traitment des fichiers\n","import bin.treatment as tr\n","\n","treatment = tr.Treatment()\n","\n","x_scaler, y_scaler = treatment.get_scalers(img_size, x_max, y_min)\n","\n","list_polygons = shapely.affinity.scale(train_polygons, xfact=x_scaler, yfact=y_scaler, origin=(0, 0, 0))\n","\n","train_mask = treatment.get_mask_polygons(img_size, list_polygons)\n","\n","img = treatment.get_img_rgb(img)\n","mask = train_mask\n","\n","#fcn = models.segmentation.fcn_resnet101(pretrained=True).eval()\n","#fcn = models.segmentation.fcn_resnet101(num_classes=2).train()\n","fcn = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=1, activation=None)\n","\n","# Apply the transformations needed\n","import torchvision.transforms as T\n","X_transforms = T.Compose([T.Resize(256),\n","                          T.CenterCrop(224),\n","                          T.ToTensor(),\n","                          T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","                          ])\n","\n","X_t = T.Compose([T.Resize(256),\n","                 T.CenterCrop(224)\n","                 ])\n","\n","Y_transforms = T.Compose([T.Resize(256),\n","                          T.CenterCrop(224),\n","                          T.ToTensor()\n","                          #T.Normalize((0.5,), (0.5,))\n","                          ])\n","\n","X_train = X_transforms(Image.fromarray(img.astype('uint8'))).unsqueeze(0)\n","Y_train = Y_transforms(Image.fromarray(mask.astype('uint8'))).unsqueeze(0)\n","#X_train = X_transforms(Image.fromarray(img.astype('uint8')))\n","#Y_train = Y_transforms(Image.fromarray(mask.astype('uint8')))\n","\n","test = X_t(Image.fromarray(img.astype('uint8')))\n","pixels = list(test.getdata())\n","width, height = test.size\n","pixels = [pixels[i * width:(i + 1) * width] for i in range(height)]\n","\n","#print(pixels)\n","#print(Y_train)\n","#plt.imshow(X_train); plt.show()\n","#plt.imshow(Y_train); plt.show()\n","\n","#criterion = nn.MSELoss(reduction='sum')\n","criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.SGD(fcn.parameters(), lr=1e-4, momentum=0.9)\n","\n","for t in range(10):\n","    # Pass the input through the net\n","    Y_pred = fcn(X_train)['out']\n","    #print(out.shape)\n","    \n","    # Compute and print loss\n","    loss = criterion(Y_pred, Y_train)\n","    #if t % 100 == 99:\n","    print(t, loss.item())\n","\n","    # Zero gradients, perform a backward pass, and update the weights.\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","def tensor_to_image(tensor, mean=0, std=1):\n","    img = np.transpose(tensor.detach().numpy(), (1, 2, 0))\n","    img = (img*std+ mean)*255\n","    img = img.astype(np.uint8)\n","    return img \n","\n","#om = transforms.ToPILImage(X_train.squeeze())\n","#om = tensor_to_image(X_train.squeeze(0))\n","\n","#om = tensor_to_image(Y_pred.squeeze(0))\n","#om = treatment.get_img_rgb(om.transpose(1,2,0))\n","\n","om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n","print(om)\n","print (om.shape)\n","#om = Image.fromarray(om)\n","#print (om.size)\n","#print(om)\n","\n","rgb = decode_segmap(om)\n","plt.imshow(rgb); plt.show()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"236d5e535b9e4571bf70b772be8d7dfc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-b9fd43579a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Pass the input through the net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;31m#print(out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"]}]}]}